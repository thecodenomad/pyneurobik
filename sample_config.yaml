model_provider: ollama
oci_provider: podman
models:
  - repo_name: TheBloke/Llama-2-7B-Chat-GGUF
    model_name: llama-2-7b-chat.Q4_K_M.gguf
    location: $HOME/models/llama-2-7b-chat.Q4_K_M.gguf
    confirmation_file: $HOME/models/.llama_downloaded
    checksum: abc123  # placeholder
oci:
  - image: docker.io/library/alpine:latest
    confirmation_file: $HOME/images/.alpine_pulled
